{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-59152712-8\"></script>\n",
    "<script>\n",
    "  window.dataLayer = window.dataLayer || [];\n",
    "  function gtag(){dataLayer.push(arguments);}\n",
    "  gtag('js', new Date());\n",
    "\n",
    "  gtag('config', 'UA-59152712-8');\n",
    "</script>\n",
    "\n",
    "# Convert LaTeX Sentence to SymPy Expression\n",
    "\n",
    "## Author: Ken Sible\n",
    "\n",
    "## The following module will demonstrate a recursive descent parser for LaTeX.\n",
    "\n",
    "### NRPy+ Source Code for this module:\n",
    "1. [latex_parser.py](../edit/latex_parser.py); [\\[**tutorial**\\]](Tutorial-LaTeX_SymPy_Conversion.ipynb) The latex_parser.py script will convert a LaTeX sentence to a SymPy expression using the following function: parse(sentence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "\n",
    "# Table of Contents\n",
    "$$\\label{toc}$$\n",
    "\n",
    "1. [Step 1](#intro): Introduction: Lexical Analysis and Syntax Analysis\n",
    "1. [Step 2](#sandbox): Demonstration and Sandbox (LaTeX Parser)\n",
    "1. [Step 3](#tensor): Tensor Support with Einstein Notation (WIP)\n",
    "1. [Step 4](#latex_pdf_output): $\\LaTeX$ PDF Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "# Step 1: Lexical Analysis and Syntax Analysis \\[Back to [top](#toc)\\]\n",
    "$$\\label{intro}$$\n",
    "\n",
    "In the following section, we discuss [lexical analysis](https://en.wikipedia.org/wiki/Lexical_analysis) (lexing) and [syntax analysis](https://en.wikipedia.org/wiki/Parsing) (parsing). In the process of lexical analysis, a lexer will tokenize a character string, called a sentence, using substring pattern matching (or tokenizing). We implemented a regex-based lexer for NRPy+, which does pattern matching using a [regular expression](https://en.wikipedia.org/wiki/Regular_expression) for each token pattern. In the process of syntax analysis, a parser will receive a token iterator from the lexer and build a parse tree containing all syntactic information of the language, as specified by a [formal grammar](https://en.wikipedia.org/wiki/Formal_grammar). We implemented a [recursive descent parser](https://en.wikipedia.org/wiki/Recursive_descent_parser) for NRPy+, which will build a parse tree in [preorder](https://en.wikipedia.org/wiki/Tree_traversal#Pre-order_(NLR)), starting from the root [nonterminal](https://en.wikipedia.org/wiki/Terminal_and_nonterminal_symbols), using a [right recursive](https://en.wikipedia.org/wiki/Left_recursion) grammar. The following right recursive, [context-free grammar](https://en.wikipedia.org/wiki/Context-free_grammar) was written for parsing [LaTeX](https://en.wikipedia.org/wiki/LaTeX), adhering to the canonical (extended) [BNF](https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form) notation used for describing a context-free grammar:\n",
    "```\n",
    "<ROOT>       -> <EXPRESSION> | <STRUCTURE> { <LINE_BREAK> <STRUCTURE> }*\n",
    "<STRUCTURE>  -> <CONFIG> | <ENVIROMENT> | <ASSIGNMENT>\n",
    "<ENVIROMENT> -> <BEGIN_ALIGN> <ASSIGNMENT> { <LINE_BREAK> <ASSIGNMENT> }* <END_ALIGN>\n",
    "<ASSIGNMENT> -> <VARIABLE> = <EXPRESSION>\n",
    "<EXPRESSION> -> <TERM> { ( '+' | '-' ) <TERM> }*\n",
    "<TERM>       -> <FACTOR> { [ '/' ] <FACTOR> }*\n",
    "<FACTOR>     -> <BASE> { '^' <EXPONENT> }*\n",
    "<BASE>       -> [ '-' ] ( <ATOM> | '(' <EXPRESSION> ')' | '[' <EXPRESSION> ']' )\n",
    "<EXPONENT>   -> <BASE> | '{' <BASE> '}'\n",
    "<ATOM>       -> <VARIABLE> | <NUMBER> | <COMMAND>\n",
    "<VARIABLE>   -> <ARRAY> | <SYMBOL> [ '_' ( <SYMBOL> | <INTEGER> ) ]\n",
    "<NUMBER>     -> <RATIONAL> | <DECIMAL> | <INTEGER>\n",
    "<COMMAND>    -> <SQRT> | <FRAC>\n",
    "<SQRT>       -> '\\\\sqrt' [ '[' <INTEGER> ']' ] '{' <EXPRESSION> '}'\n",
    "<FRAC>       -> '\\\\frac' '{' <EXPRESSION> '}' '{' <EXPRESSION> '}'\n",
    "<CONFIG>     -> '%' <ARRAY> '[' <INTEGER> ']' [ ':' <SYMMETRY> ] { ',' <ARRAY> '[' <INTEGER> ']' [ ':' <SYMMETRY> ] }*\n",
    "<ARRAY>      -> ( <SYMBOL | <TENSOR> ) \n",
    "                    [ '_' ( <SYMBOL> | '{' { <SYMBOL> }+ '}' ) [ '^' ( <SYMBOL> | '{' { <SYMBOL> }+ '}' ) ]\n",
    "                    | '^' ( <SYMBOL> | '{' { <SYMBOL> }+ '}' ) [ '_' ( <SYMBOL> | '{' { <SYMBOL> }+ '}' ) ] ]\n",
    "```\n",
    "\n",
    "<small>**Source**: Robert W. Sebesta. Concepts of Programming Languages. Pearson Education Limited, 2016.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latex_parser import * # Import NRPy+ module for lexing and parsing LaTeX\n",
    "from sympy import srepr    # Import SymPy function for expression tree representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQRT_CMD, LEFT_BRACE, INTEGER, RIGHT_BRACE, LEFT_PAREN, SYMBOL, PLUS, RATIONAL, RIGHT_PAREN, CARET, INTEGER\n"
     ]
    }
   ],
   "source": [
    "lexer = Lexer(); lexer.initialize(r'\\sqrt{5}(x + 2/3)^2')\n",
    "print(', '.join(token for token in lexer.tokenize()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqrt(5)*(x + 2/3)**2 : Mul(Pow(Integer(5), Rational(1, 2)), Pow(Add(Symbol('x'), Rational(2, 3)), Integer(2)))\n"
     ]
    }
   ],
   "source": [
    "expr = parse(r'\\sqrt{5}(x + 2/3)^2', expression=True)\n",
    "print(expr, ':', srepr(expr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Grammar Derivation: (x + 2/3)^2`\n",
    "```\n",
    "<EXPRESSION> -> <TERM>\n",
    "             -> <FACTOR>\n",
    "             -> <BASE>^<EXPONENT>\n",
    "             -> (<EXPRESSION>)^<EXPONENT>\n",
    "             -> (<TERM> + <TERM>)^<EXPONENT>\n",
    "             -> (<FACTOR> + <TERM>)^<EXPONENT>\n",
    "             -> (<BASE> + <TERM>)^<EXPONENT>\n",
    "             -> (<ATOM> + <TERM>)^<EXPONENT>\n",
    "             -> (<VARIABLE> + <TERM>)^<EXPONENT>\n",
    "             -> (<SYMBOL> + <TERM>)^<EXPONENT>\n",
    "             -> (x + <TERM>)^<EXPONENT>\n",
    "             -> (x + <FACTOR>)^<EXPONENT>\n",
    "             -> (x + <BASE>)^<EXPONENT>\n",
    "             -> (x + <ATOM>)^<EXPONENT>\n",
    "             -> (x + <NUMBER>)^<EXPONENT>\n",
    "             -> (x + <RATIONAL>)^<EXPONENT>\n",
    "             -> (x + 2/3)^<EXPONENT>\n",
    "             -> (x + 2/3)^<BASE>\n",
    "             -> (x + 2/3)^<ATOM>\n",
    "             -> (x + 2/3)^<NUMBER>\n",
    "             -> (x + 2/3)^<INTEGER>\n",
    "             -> (x + 2/3)^2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sandbox'></a>\n",
    "\n",
    "# Step 2: Demonstration and Sandbox (LaTeX Parser) \\[Back to [top](#toc)\\]\n",
    "$$\\label{sandbox}$$\n",
    "\n",
    "We implemented a wrapper function for the `parse()` method that will accept a LaTeX sentence and return a SymPy expression. Furthermore, the entire parsing module was designed for extendibility. We apply the following procedure for extending parser functionality to include an unsupported LaTeX command: append that command to the grammar dictionary in the Lexer class with the mapping regex:token, write a grammar abstraction (similar to a regular expression) for that command, add the associated nonterminal (the command name) to the command abstraction in the Parser class, and finally implement the straightforward (private) method for parsing the grammar abstraction. We shall demonstrate the extension procedure using the `\\sqrt` LaTeX command.\n",
    "\n",
    "```<SQRT> -> '\\\\sqrt' [ '[' <INTEGER> ']' ] '{' <EXPRESSION> '}'```\n",
    "```\n",
    "def _sqrt(self):\n",
    "    if self.accept('LEFT_BRACKET'):\n",
    "        integer = self.lexer.lexeme\n",
    "        self.expect('INTEGER')\n",
    "        root = Rational(1, integer)\n",
    "        self.expect('RIGHT_BRACKET')\n",
    "    else: root = Rational(1, 2)\n",
    "    self.expect('LEFT_BRACE')\n",
    "    expr = self.__expr()\n",
    "    self.expect('RIGHT_BRACE')\n",
    "    return Pow(expr, root)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_0**(1/3)\n"
     ]
    }
   ],
   "source": [
    "print(parse(r'\\sqrt[3]{\\alpha_0}', expression=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to expression parsing, we included support for equation parsing, which will produce a dictionary mapping LHS $\\mapsto$ RHS, where LHS must be a symbol, and insert that mapping into the global namespace of the previous stack frame, as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2**(n/2)*n\n"
     ]
    }
   ],
   "source": [
    "parse(r'x = n\\sqrt{2}^n'); print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented robust error messaging using the custom `ParseError` exception, which should handle every conceivable case to identify, as detailed as possible, invalid syntax inside of a LaTeX sentence. The following are runnable examples of possible error messages (simply uncomment and run the cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse(r'\\sqrt[*]{2}')\n",
    "    # ParseError: \\sqrt[*]{2}\n",
    "    #                   ^\n",
    "    # unexpected '*' at position 6\n",
    "\n",
    "# parse(r'\\sqrt[0.5]{2}')\n",
    "    # ParseError: \\sqrt[0.5]{2}\n",
    "    #                   ^\n",
    "    # expected token INTEGER at position 6\n",
    "\n",
    "# parse(r'\\command{}')\n",
    "    # ParseError: \\command{}\n",
    "    #             ^\n",
    "    # unsupported command '\\command' at position 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings # Import Python function for warning suppression\n",
    "filterwarnings('ignore', category=OverrideWarning); del Parser.namespace['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sandbox code cell below, you can experiment with the LaTeX parser using the wrapper function `parse(sentence)`, where sentence must be a [raw string](https://docs.python.org/3/reference/lexical_analysis.html) to interpret a backslash as a literal character rather than an [escape sequence](https://en.wikipedia.org/wiki/Escape_sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Sandbox Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tensor'></a>\n",
    "\n",
    "# Step 3: Tensor Support with Einstein Notation (WIP) \\[Back to [top](#toc)\\]\n",
    "$$\\label{tensor}$$\n",
    "\n",
    "In the following section, we demonstrate the current parser support for tensor notation using the Einstein summation convention. The first example will parse an equation for a tensor contraction, the second will parse an equation for raising an index using the metric tensor, and the third will parse an align enviroment with an equation dependency. In each example, every tensor should appear either on the LHS of an equation or inside of a configuration before appearing on the RHS of an equation. Moreover, the parser will raise an exception upon violation of the Einstein summation convention, i.e. an invalid free or bound index.\n",
    "\n",
    "**Configuration Syntax** `% <TENSOR> [<DIMENSION>]: <SYMMETRY>, <TENSOR> [<DIMENSION>]: <SYMMETRY>, ... ;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1\n",
    "LaTeX Source | Rendered LaTeX\n",
    ":----------- | :-------------\n",
    "<pre lang=\"latex\"> h = h^\\\\mu{}_\\\\mu </pre> | $$ h = h^\\mu{}_\\mu $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hUD', 'h']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(r\"\"\"\n",
    "    % h^\\mu_\\mu [4]: nosym;\n",
    "    h = h^\\mu{}_\\mu\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h = hUD00 + hUD11 + hUD22 + hUD33\n"
     ]
    }
   ],
   "source": [
    "print('h =', h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2\n",
    "LaTeX Source | Rendered LaTeX\n",
    ":----------- | :-------------\n",
    "<pre lang=\"latex\"> v^\\\\mu = g^{\\\\mu\\\\nu}v_\\\\nu </pre> | $$ v^\\mu = g^{\\mu\\nu}v_\\nu $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gUU', 'gDD', 'vD', 'vU']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(r\"\"\"\n",
    "    % g^{\\mu\\nu} [3]: metric, v_\\nu [3];\n",
    "    v^\\mu = g^{\\mu\\nu}v_\\nu\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vU = [gUU00*vD0 + gUU01*vD1 + gUU02*vD2, gUU01*vD0 + gUU11*vD1 + gUU12*vD2, gUU02*vD0 + gUU12*vD1 + gUU22*vD2]\n"
     ]
    }
   ],
   "source": [
    "print('vU =', vU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3\n",
    "LaTeX Source | Rendered LaTeX\n",
    ":----------- | :-------------\n",
    "<pre lang=\"latex\"> \\\\begin{align\\*}<br>&emsp;&emsp;&emsp; R &= g_{ab}R^{ab} \\\\\\\\ <br>&emsp;&emsp;&emsp; G^{ab} &= R^{ab} - \\\\frac{1}{2}g^{ab}R <br> \\\\end{align\\*} </pre> | $$ \\begin{align*} R &= g_{ab}R^{ab} \\\\ G^{ab} &= R^{ab} - \\frac{1}{2}g^{ab}R \\end{align*} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gDD', 'gUU', 'RUU', 'R', 'GUU']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(r\"\"\"\n",
    "    % g_{ab} [2]: metric, R^{ab} [2]: sym01;\n",
    "    \\begin{align*}\n",
    "        R &= g_{ab}R^{ab} \\\\\n",
    "        G^{ab} &= R^{ab} - \\frac{1}{2}g^{ab}R\n",
    "    \\end{align*}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R = RUU00*gDD00 + 2*RUU01*gDD01 + RUU11*gDD11\n"
     ]
    }
   ],
   "source": [
    "print('R =', R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[RUU00 + gDD11*(-RUU00*gDD00 - 2*RUU01*gDD01 - RUU11*gDD11)/(2*(gDD00*gDD11 - gDD01**2)),\n",
       "  RUU01 - gDD01*(-RUU00*gDD00 - 2*RUU01*gDD01 - RUU11*gDD11)/(2*(gDD00*gDD11 - gDD01**2))],\n",
       " [RUU01 - gDD01*(-RUU00*gDD00 - 2*RUU01*gDD01 - RUU11*gDD11)/(2*(gDD00*gDD11 - gDD01**2)),\n",
       "  RUU11 + gDD00*(-RUU00*gDD00 - 2*RUU01*gDD01 - RUU11*gDD11)/(2*(gDD00*gDD11 - gDD01**2))]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(GUU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The static variable `namespace` for the `Parser` class will provide access to the global namespace of the parser across each instance of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hUD',\n",
       "              [[hUD00, hUD01, hUD02, hUD03],\n",
       "               [hUD10, hUD11, hUD12, hUD13],\n",
       "               [hUD20, hUD21, hUD22, hUD23],\n",
       "               [hUD30, hUD31, hUD32, hUD33]]),\n",
       "             ('h', hUD00 + hUD11 + hUD22 + hUD33),\n",
       "             ('gUU',\n",
       "              [[gDD11/(gDD00*gDD11 - gDD01**2),\n",
       "                -gDD01/(gDD00*gDD11 - gDD01**2)],\n",
       "               [-gDD01/(gDD00*gDD11 - gDD01**2),\n",
       "                gDD00/(gDD00*gDD11 - gDD01**2)]]),\n",
       "             ('gDD', [[gDD00, gDD01], [gDD01, gDD11]]),\n",
       "             ('vD', [vD0, vD1, vD2]),\n",
       "             ('vU',\n",
       "              [gUU00*vD0 + gUU01*vD1 + gUU02*vD2,\n",
       "               gUU01*vD0 + gUU11*vD1 + gUU12*vD2,\n",
       "               gUU02*vD0 + gUU12*vD1 + gUU22*vD2]),\n",
       "             ('RUU', [[RUU00, RUU01], [RUU01, RUU11]]),\n",
       "             ('R', RUU00*gDD00 + 2*RUU01*gDD01 + RUU11*gDD11),\n",
       "             ('GUU',\n",
       "              [[RUU00 + gDD11*(-RUU00*gDD00 - 2*RUU01*gDD01 - RUU11*gDD11)/(2*(gDD00*gDD11 - gDD01**2)),\n",
       "                RUU01 - gDD01*(-RUU00*gDD00 - 2*RUU01*gDD01 - RUU11*gDD11)/(2*(gDD00*gDD11 - gDD01**2))],\n",
       "               [RUU01 - gDD01*(-RUU00*gDD00 - 2*RUU01*gDD01 - RUU11*gDD11)/(2*(gDD00*gDD11 - gDD01**2)),\n",
       "                RUU11 + gDD00*(-RUU00*gDD00 - 2*RUU01*gDD01 - RUU11*gDD11)/(2*(gDD00*gDD11 - gDD01**2))]])])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parser.namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extended our robust error messaging using the custom `TensorError` exception, which should handle any inconsistent tensor dimension and any violation of the Einstein summation convention, specifically that a bound index must appear exactly once as a superscript and exactly once as a subscript in any single term and that a free index must appear in every term with the same position and cannot be summed over in any term. The following are runnable examples of possible error messages (simply uncomment and run the cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse(r\"\"\"\n",
    "#     % h^{\\mu\\mu}_{\\mu\\mu} [4]: nosym;\n",
    "#     h = h^{\\mu\\mu}_{\\mu\\mu}\n",
    "# \"\"\")\n",
    "    # TensorError: illegal bound index\n",
    "\n",
    "# parse(r\"\"\"\n",
    "#     % g^\\mu_\\nu [3]: sym01, v_\\nu [3];\n",
    "#     v^\\mu = g^\\mu_\\nu v_\\nu\n",
    "# \"\"\")\n",
    "    # TensorError: illegal bound index\n",
    "\n",
    "# parse(r\"\"\"\n",
    "#     % g^{\\mu\\nu} [3]: sym01, v_\\mu [3], w_\\nu [3];\n",
    "#     u^\\mu = g^{\\mu\\nu}(v_\\mu + w_\\nu)\n",
    "# \"\"\")\n",
    "    # TensorError: unbalanced free index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='latex_pdf_output'></a>\n",
    "\n",
    "# Step 4: Output this notebook to $\\LaTeX$-formatted PDF file \\[Back to [top](#toc)\\]\n",
    "$$\\label{latex_pdf_output}$$\n",
    "\n",
    "The following code cell converts this Jupyter notebook into a proper, clickable $\\LaTeX$-formatted PDF file. After the cell is successfully run, the generated PDF may be found in the root NRPy+ tutorial directory, with filename\n",
    "[Tutorial-LaTeX_SymPy_Conversion.pdf](Tutorial-LaTeX_SymPy_Conversion.pdf) (Note that clicking on this link may not work; you may need to open the PDF file through another means.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Tutorial-LaTeX_SymPy_Conversion.tex, and compiled LaTeX file to PDF\n",
      "    file Tutorial-LaTeX_SymPy_Conversion.pdf\n"
     ]
    }
   ],
   "source": [
    "import cmdline_helper as cmd    # NRPy+: Multi-platform Python command-line interface\n",
    "cmd.output_Jupyter_notebook_to_LaTeXed_PDF(\"Tutorial-LaTeX_SymPy_Conversion\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
